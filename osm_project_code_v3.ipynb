{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "from xml.etree.ElementTree import parse\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "import sqlite3\n",
    "from odo import discover, resource, odo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference to Data Auditing process (project_submission_doc.pdf)\n",
    "# 1) Provide a programmable reference to correct data\n",
    "## REF1\n",
    "import normalized_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference to Data Auditing process (project_submission_doc.pdf)\n",
    "# 2) Sample the OSM file and clean station names\n",
    "actual_stations = []\n",
    "decoded_stations = {}\n",
    "\n",
    "## REF2\n",
    "def parse_and_collect_nodes(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    doc = ET.iterparse(osm_file, events=(\"start\",\"end\"))\n",
    "    try:\n",
    "        for event, elem in doc:\n",
    "            if event == \"start\" and elem.tag == \"node\":\n",
    "                process_every_node(elem.iter(\"tag\"))\n",
    "            elif event == \"end\":\n",
    "                continue\n",
    "    except:\n",
    "        print \"End of file reached\"\n",
    "## REF3\n",
    "def process_every_node(itero):\n",
    "    k_tags = []\n",
    "    v_tags = []\n",
    "    for tag in itero:\n",
    "        k_tags.append(tag.attrib['k']) \n",
    "        v_tags.append(tag.attrib['v'])\n",
    "    extract_stations(k_tags, v_tags)  \n",
    "\n",
    "## REF4\n",
    "def extract_stations(k_tags, v_tags):\n",
    "    if \"subway\" in k_tags:\n",
    "        actual_stations.append(v_tags[0])\n",
    "\n",
    "## REF5\n",
    "def to_ascii_actual_stations():\n",
    "    for index, station in enumerate(actual_stations):\n",
    "        # decoded_stations.append(unidecode(station))\n",
    "        decoded_stations[index] = unidecode(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reference to Data Auditing process (project_submission_doc.pdf)\n",
    "# 3) First pass stations and the rest and 4) Dealing with the “no match” stations names\n",
    "match_stations   = {}\n",
    "nomatch_stations = {}\n",
    "dict_of_tokeners = {}\n",
    "final_review_nomatch = []\n",
    "\n",
    "## REF7\n",
    "\n",
    "tokens_to_ignore = [\" de \", \" los \", \"-\", \" La \", \" Los \", \"los\", \"\\(E\\)\", \"\\(C\\)\", \"\\(D\\)\"]\n",
    "## split each station name in final_normalization in tokens ignoring the ones in tokens_to_ignore\n",
    "pattern = '|'.join(tokens_to_ignore)\n",
    "\n",
    "# first pass of the actual_stations collected from OSM through a compare with official stations\n",
    "## REF6\n",
    "def split_match_nomatch_stations():\n",
    "    for key in decoded_stations.keys():\n",
    "        if decoded_stations[key] in normalized_stations.all_stations():\n",
    "            # print \"found : {}, index: {}\".format(decoded_stations[key],key)\n",
    "            match_stations[key] = decoded_stations[key]\n",
    "        else:\n",
    "            nomatch_stations[key] = decoded_stations[key]\n",
    "\n",
    "def split_nomatch():\n",
    "    for key in nomatch_stations.keys():        \n",
    "        tokeners = re.sub(pattern, ' ', nomatch_stations[key])\n",
    "        dict_of_tokeners[key] = tokeners\n",
    "\n",
    "def review_nomatch(tokener, key):\n",
    "    nomatch_dict_key = key\n",
    "    tokis = tokener.split()\n",
    "    match_counter = defaultdict(int)\n",
    "    for station in normalized_stations.all_stations():\n",
    "        for tok in tokis:\n",
    "            if tok in station:\n",
    "                match_counter[station] += 1\n",
    "                datum = {\"nomatch_key\" : nomatch_dict_key, \"norm_station\" : station, \"nomatch_station\" : nomatch_stations[nomatch_dict_key], \"count\" : match_counter[station]}\n",
    "                final_review_nomatch.append(datum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = \"bs_as_subway.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Auditing process execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of file reached\n"
     ]
    }
   ],
   "source": [
    "parse_and_collect_nodes(OSMFILE)\n",
    "# calls process_every_node()\n",
    "# --> calls extract_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tano/dataAnalysis/anaconda2/envs/dataviz/lib/python2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    }
   ],
   "source": [
    "to_ascii_actual_stations()\n",
    "split_match_nomatch_stations()\n",
    "split_nomatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in dict_of_tokeners.keys():\n",
    "    review_nomatch(dict_of_tokeners[key], key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 4) Dealing with the “no match” stations names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start building dicts to export to csv and to sqlite3\n",
    "# stations that did not match\n",
    "nomatch_stations_dict_list = []\n",
    "for dat in nomatch_stations.keys():\n",
    "    datum = {\"station_id\" : dat, \"station_name\" : nomatch_stations[dat], \"match\" : \"no\"}\n",
    "    nomatch_stations_dict_list.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stations that did match\n",
    "match_stations_dict_list = []\n",
    "for dat in match_stations.keys():\n",
    "    datum = {\"station_id\" : dat, \"station_name\" : match_stations[dat], \"match\" : \"yes\"}\n",
    "    match_stations_dict_list.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The sum of match and nomatch stations decoded\n",
    "decoded_stations_dict_list = []\n",
    "for dat in decoded_stations.keys():\n",
    "    datum = {\"station_id\" : dat, \"station_name\" : decoded_stations[dat]}\n",
    "    decoded_stations_dict_list.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The official list of stations and lines\n",
    "all_stations_dict_list = []\n",
    "\n",
    "def process_stations_by_lines(line, stations):\n",
    "    for station in stations:\n",
    "        dico = {\"station_name\"  : station, \"line\" : line}\n",
    "        all_stations_dict_list.append(dico)\n",
    "\n",
    "\n",
    "lines = [\"A\",\"B\",\"C\",\"D\",\"E\",\"H\",\"P\"]\n",
    "for line in lines:\n",
    "    stations = []\n",
    "    stations = normalized_stations.stations_by_line(line)\n",
    "    process_stations_by_lines(line, stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The csv files to export  to sqlite3\n",
    "## REF8\n",
    "\n",
    "NOMATCH_STATIONS_TOKENS_PATH = \"nomatch_tokens.csv\" # final_review_nomatch\n",
    "NOMATCH_STATIONS_PATH        = \"nomatch.csv\" # nomatch_stations_dict_list\n",
    "MATCH_STATIONS_PATH          = \"match.csv\"   # match_stations\n",
    "DECODED_STATIONS_PATH        = \"decoded.csv\" # decoded_stations\n",
    "OFICIAL_STATIONS_PATH        = \"official.csv\" # all_stations_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fields for tables\n",
    "NOMATCH_STATIONS_TOKENS_FIELDS = ['count','norm_station','nomatch_station','nomatch_key'] # final_review_nomatch\n",
    "NOMATCH_STATIONS_PATH_FIELDS   = ['station_name','match','station_id'] # nomatch_stations\n",
    "MATCH_STATIONS_PATH_FIELDS     = ['station_name','match','station_id']   # match_stations\n",
    "DECODED_STATIONS_PATH_FIELDS   = ['station_name','station_id'] # decoded_stations\n",
    "OFICIAL_STATIONS_PATH_FIELDS   = ['station_name','line'] # all_stations_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csv files creation function\n",
    "\n",
    "def create_csv_files(path, fields, dic):\n",
    "    with open(path, 'w') as csvfile:\n",
    "        fieldnames = fields\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_csv_files(OFICIAL_STATIONS_PATH, OFICIAL_STATIONS_PATH_FIELDS, all_stations_dict_list)\n",
    "create_csv_files(NOMATCH_STATIONS_TOKENS_PATH, NOMATCH_STATIONS_TOKENS_FIELDS, final_review_nomatch)\n",
    "create_csv_files(NOMATCH_STATIONS_PATH, NOMATCH_STATIONS_PATH_FIELDS, nomatch_stations_dict_list)\n",
    "create_csv_files(MATCH_STATIONS_PATH, MATCH_STATIONS_PATH_FIELDS, match_stations_dict_list)\n",
    "create_csv_files(DECODED_STATIONS_PATH, DECODED_STATIONS_PATH_FIELDS, decoded_stations_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f61d05d5570>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, now its time to build the db and the tables\n",
    "conn = sqlite3.connect('subway.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''create table nomatch_tokens (count integer, norm_station text, nomatch_station text, nomatch_key integer)''')\n",
    "c.execute('''create table nomatch_stations (station_name text, match text, station_id integer)''')\n",
    "c.execute('''create table match_stations (station_name text, match text, station_id integer)''')\n",
    "c.execute('''create table decoded_stations (station_name text, station_id integer)''')\n",
    "c.execute('''create table official_stations (station_name text, line text)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv to db \n",
    "def csv_to_db(csv_file, table_name, db_con='subway.db'):\n",
    "    dshape = discover(resource(csv_file))\n",
    "    uri = 'sqlite:///' + db_con + '::' + table_name\n",
    "    try :\n",
    "        odo(csv_file, uri, dshape=dshape)\n",
    "        print \"Data loaded\"\n",
    "    except:\n",
    "        print \"Problems loading data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data loaded\n",
      "Data loaded\n",
      "Data loaded\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "csv_to_db(NOMATCH_STATIONS_TOKENS_PATH, 'nomatch_tokens')\n",
    "csv_to_db(NOMATCH_STATIONS_PATH, 'nomatch_stations')\n",
    "csv_to_db(MATCH_STATIONS_PATH , 'match_stations')\n",
    "csv_to_db(DECODED_STATIONS_PATH, 'decoded_stations')\n",
    "csv_to_db(OFICIAL_STATIONS_PATH , 'official_stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SQL query to find out which no matching tokenized station name is the best candidate\n",
    "## REF9\n",
    "\n",
    "QUERY = '''\n",
    "select nomatch_tokens.nomatch_station as normalizar,\n",
    "count(nomatch_tokens.norm_station) as match,\n",
    "official_stations.station_name as normal\n",
    "from nomatch_tokens, official_stations\n",
    "where nomatch_tokens.norm_station = official_stations.station_name\n",
    "and nomatch_tokens.nomatch_key = %d\n",
    "group by nomatch_tokens.norm_station\n",
    "order by match desc\n",
    "limit 1;\n",
    "'''\n",
    "\n",
    "# find out the ID of the nonmatching stations\n",
    "NOMATCH_IDS = '''\n",
    "select distinct nomatch_key from nomatch_tokens;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Plaza de los Virreyes - Eva Peron', 4, u'Plaza de los Virreyes Eva Peron')\n",
      "(u'Avenida La Plata', 1, u'Av. La Plata')\n",
      "(u'Entre Rios - Rodolfo Walsh', 4, u'Entre Rios Rodolfo Walsh')\n",
      "(u'Independencia (E)', 4, u'Independencia')\n",
      "(u'Independencia (C)', 4, u'Independencia')\n",
      "(u'General Urquiza', 1, u'General San Martin')\n",
      "(u'Malabia - Osvaldo Pugliese', 2, u'Malabia O. Pugliese')\n",
      "(u'Pasteur - AMIA', 1, u'Pasteur Amia')\n",
      "(u'Inclan-Mezquita Al Ahmad', 1, u'Alberti')\n",
      "(u'Medrano-Almagro', 1, u'Medrano')\n",
      "(u'Tribunales', 1, u'Tribunales Teatro Colon')\n",
      "(u'Callao (D)', 4, u'Callao')\n",
      "(u'Federico Lacroze', 1, u'F. Lacroze')\n",
      "(u'Tronador - Villa Ortuzar', 3, u'Tronador Villa Ortuzar')\n",
      "(u'Los Incas - Parque Chas', 3, u'De los Incas Parque Chas')\n",
      "(u'Pueyrredon (D)', 4, u'Pueyrredon')\n"
     ]
    }
   ],
   "source": [
    "# process the list of nonmatchin ids and tokenized versions to list the candidates\n",
    "candidates_ids = []\n",
    "candidates_list = []\n",
    "\n",
    "def search_nomatch_candidates(cursor):\n",
    "    for nmid in candidates_ids:\n",
    "        nomatch_id = nmid\n",
    "        cursor.execute(QUERY%(nomatch_id))\n",
    "        nomatch_string = cursor.fetchone()\n",
    "        candidates_list.append(nomatch_string)\n",
    "    \n",
    "\n",
    "def search_nomatch_ids(cursor):\n",
    "    cursor.execute(NOMATCH_IDS)\n",
    "    results = cursor.fetchall()\n",
    "    for tup in results:\n",
    "        candidates_ids.append(tup[0])\n",
    "        \n",
    "        \n",
    "search_nomatch_ids(c)\n",
    "search_nomatch_candidates(c)\n",
    "# And finally the list of candidates that match !\n",
    "\n",
    "for cand in candidates_list:\n",
    "    print cand\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
